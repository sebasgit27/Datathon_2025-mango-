{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#  Mango Datathon – LGBM con ventas/demanda agregadas\n",
        "#  - Target: Production\n",
        "#  - Usa weekly_sales / weekly_demand agregadas por ID\n",
        "#  - Tuning de num_leaves (coarse + fine)\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/Datathon\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Cargar datos + renombrar columnas Unnamed del test\n",
        "# -------------------------------------------------------------------\n",
        "def load_data():\n",
        "    train_path = DATA_DIR / \"train.csv\"\n",
        "    test_path = DATA_DIR / \"test.csv\"\n",
        "    sample_sub_path = DATA_DIR / \"sample_submission.csv\"\n",
        "\n",
        "    print(\"Leyendo CSV...\")\n",
        "    train = pd.read_csv(train_path, sep=';')\n",
        "    test = pd.read_csv(test_path, sep=';')\n",
        "    sample_sub = pd.read_csv(sample_sub_path)\n",
        "\n",
        "    # Renombrar columnas Unnamed del test para que coincidan con train\n",
        "    unnamed_test_cols = [c for c in test.columns if c.startswith(\"Unnamed\")]\n",
        "    if unnamed_test_cols:\n",
        "        n_unnamed = len(unnamed_test_cols)\n",
        "        last_train_cols = train.columns[-n_unnamed:]\n",
        "        rename_map = {old: new for old, new in zip(unnamed_test_cols, last_train_cols)}\n",
        "        print(\"Renombrando columnas Unnamed en test.csv:\")\n",
        "        print(rename_map)\n",
        "        test = test.rename(columns=rename_map)\n",
        "\n",
        "    print(\"train shape:\", train.shape)\n",
        "    print(\"test shape:\", test.shape)\n",
        "    print(\"sample_submission shape:\", sample_sub.shape)\n",
        "\n",
        "    return train, test, sample_sub\n",
        "\n",
        "\n",
        "# (Esta función queda por si quieres usar embeddings más adelante, ahora no se usa)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def parse_embedding(df, column_name='image_embedding'):\n",
        "    \"\"\"Convierte la columna de embedding (string) en un DataFrame de floats.\"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "\n",
        "    df[column_name] = df[column_name].astype(str).str.replace('[', '').str.replace(']', '')\n",
        "    embedding_df = df[column_name].str.split(',', expand=True).astype(float)\n",
        "    num_components = embedding_df.shape[1]\n",
        "    embedding_df.columns = [f'embed_{i}' for i in range(num_components)]\n",
        "    embedding_df = embedding_df.fillna(0)\n",
        "    return embedding_df\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Preparar features agregadas por ID\n",
        "# -------------------------------------------------------------------\n",
        "def prepare_features(train: pd.DataFrame, test: pd.DataFrame, sample_sub: pd.DataFrame):\n",
        "    if \"Production\" not in train.columns:\n",
        "        raise ValueError(\"Falta la columna 'Production' en train.csv\")\n",
        "\n",
        "    # 1) Agregaciones por ID en TRAIN\n",
        "    agg = train.groupby(\"ID\").agg(\n",
        "        weekly_sales_sum=(\"weekly_sales\", \"sum\"),\n",
        "        weekly_sales_mean=(\"weekly_sales\", \"mean\"),\n",
        "        weekly_sales_std=(\"weekly_sales\", \"std\"),             # volatilidad de ventas\n",
        "        weekly_demand_median=(\"weekly_demand\", \"median\"),     # mediana de demanda\n",
        "        demand_sales_ratio=(\"weekly_demand\",\n",
        "                            lambda x: (x.sum() /\n",
        "                                       (train.loc[x.index, \"weekly_sales\"].sum()\n",
        "                                        if train.loc[x.index, \"weekly_sales\"].sum() != 0\n",
        "                                        else 1.0)))           # ratio demanda/venta\n",
        "    ).reset_index()\n",
        "\n",
        "    # 2) Dataset a nivel producto en TRAIN (una fila por ID)\n",
        "    train_prod = train.drop_duplicates(\"ID\").merge(agg, on=\"ID\", how=\"left\")\n",
        "\n",
        "    # 3) Dataset a nivel producto en TEST alineado con sample_submission (mismo orden de ID)\n",
        "    test_uniq = test.drop_duplicates(\"ID\")\n",
        "    test_prod = sample_sub[[\"ID\"]].merge(test_uniq, on=\"ID\", how=\"left\")\n",
        "    test_prod = test_prod.merge(agg, on=\"ID\", how=\"left\")  # algunos ID nuevos pueden tener NaN en las agregadas\n",
        "\n",
        "    # 4) Definir y, X, X_test\n",
        "    y = train_prod[\"Production\"].astype(float)\n",
        "\n",
        "    drop_cols = [\n",
        "        \"Production\",\n",
        "        \"weekly_sales\",\n",
        "        \"weekly_demand\",\n",
        "        \"num_week_iso\",\n",
        "        \"year\",\n",
        "    ]\n",
        "\n",
        "    # Quitamos image_embedding por velocidad (si luego quieres meterlo, lo tratamos aparte)\n",
        "    if \"image_embedding\" in train_prod.columns:\n",
        "        drop_cols.append(\"image_embedding\")\n",
        "\n",
        "    X = train_prod.drop(columns=[c for c in drop_cols if c in train_prod.columns])\n",
        "    X_test = test_prod.drop(columns=[c for c in drop_cols if c in test_prod.columns])\n",
        "\n",
        "    # Aseguramos mismas columnas en train y test\n",
        "    X_test = X_test[X.columns]\n",
        "\n",
        "    # Detectar categóricas\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
        "\n",
        "    # Convertir a categoría para LGBM\n",
        "    for c in cat_cols:\n",
        "        X[c] = X[c].astype(\"category\")\n",
        "        X_test[c] = X_test[c].astype(\"category\")\n",
        "\n",
        "    print(\"Número de features:\", X.shape[1])\n",
        "    print(\"Categóricas:\", len(cat_cols))\n",
        "\n",
        "    return X, y, X_test, cat_cols\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Entrenamiento + validación con tuning de num_leaves\n",
        "# -------------------------------------------------------------------\n",
        "def train_and_evaluate_lgbm(\n",
        "    X, y, cat_cols,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    leaves_grid=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Entrena varios modelos LightGBM variando num_leaves y devuelve\n",
        "    el mejor modelo según RMSE de validación junto con el mejor num_leaves.\n",
        "    \"\"\"\n",
        "    if leaves_grid is None:\n",
        "        leaves_grid = [15, 31, 63, 127]\n",
        "\n",
        "    print(\"\\n[VALIDACIÓN] Haciendo train/valid split...\")\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(\"X_train:\", X_train.shape, \" X_valid:\", X_valid.shape)\n",
        "\n",
        "    lgb_train_base = lgb.Dataset(\n",
        "        X_train, label=y_train,\n",
        "        categorical_feature=cat_cols,\n",
        "        free_raw_data=False\n",
        "    )\n",
        "    lgb_valid_base = lgb.Dataset(\n",
        "        X_valid, label=y_valid,\n",
        "        categorical_feature=cat_cols,\n",
        "        free_raw_data=False\n",
        "    )\n",
        "\n",
        "    best_rmse = float(\"inf\")\n",
        "    best_model = None\n",
        "    best_leaves = None\n",
        "\n",
        "    for num_leaves in leaves_grid:\n",
        "        print(f\"\\n[VALIDACIÓN] Probando num_leaves = {num_leaves} ...\")\n",
        "\n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"learning_rate\": 0.005,\n",
        "            \"num_leaves\": num_leaves,\n",
        "            \"feature_fraction\": 0.8,\n",
        "            \"bagging_fraction\": 0.8,\n",
        "            \"bagging_freq\": 1,\n",
        "            \"verbose\": -1,\n",
        "            \"seed\": 42,\n",
        "        }\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            lgb_train_base,\n",
        "            valid_sets=[lgb_train_base, lgb_valid_base],\n",
        "            valid_names=[\"train\", \"valid\"],\n",
        "            num_boost_round=100000,\n",
        "            callbacks=[lgb.early_stopping(100, verbose=100)],\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
        "        print(f\"[RESULTADO] num_leaves={num_leaves} -> RMSE valid: {rmse:.4f}, best_iter={model.best_iteration}\")\n",
        "\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model = model\n",
        "            best_leaves = num_leaves\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"MEJOR num_leaves: {best_leaves} con RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"Mejor iteración: {best_model.best_iteration}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    return best_model, best_leaves\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Entrenar con todo el train y generar submission\n",
        "# -------------------------------------------------------------------\n",
        "def train_full_and_predict_lgbm(\n",
        "    X, y, X_test,\n",
        "    cat_cols,\n",
        "    sample_sub,\n",
        "    best_iteration,\n",
        "    best_num_leaves\n",
        "):\n",
        "    print(\"\\n[FINAL] Entrenando modelo LightGBM con TODO el train...\")\n",
        "\n",
        "    lgb_full = lgb.Dataset(\n",
        "        X, label=y,\n",
        "        categorical_feature=cat_cols,\n",
        "        free_raw_data=False\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        \"objective\": \"regression\",\n",
        "        \"metric\": \"rmse\",\n",
        "        \"learning_rate\": 0.005,\n",
        "        \"num_leaves\": best_num_leaves,\n",
        "        \"feature_fraction\": 0.8,\n",
        "        \"bagging_fraction\": 0.8,\n",
        "        \"bagging_freq\": 1,\n",
        "        \"verbose\": -1,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        "\n",
        "    model_full = lgb.train(\n",
        "        params,\n",
        "        lgb_full,\n",
        "        num_boost_round=best_iteration\n",
        "    )\n",
        "\n",
        "    print(\"[FINAL] Prediciendo sobre test...\")\n",
        "    preds = model_full.predict(X_test, num_iteration=best_iteration)\n",
        "    preds = np.maximum(preds, 0)  # evitar negativos\n",
        "\n",
        "    submission = sample_sub.copy()\n",
        "    submission[\"Production\"] = preds\n",
        "\n",
        "    return submission[[\"ID\", \"Production\"]]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Ejecutar\n",
        "# -------------------------------------------------------------------\n",
        "train, test, sample_sub = load_data()\n",
        "\n",
        "X, y, X_test, cat_cols = prepare_features(train, test, sample_sub)\n",
        "\n",
        "print(\"\\n==== VALIDACIÓN RÁPIDA CON LGBM + TUNING GRUESO num_leaves ====\")\n",
        "# 1) Barrido grueso\n",
        "model_val_coarse, best_leaves_coarse = train_and_evaluate_lgbm(\n",
        "    X, y, cat_cols,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    leaves_grid=[15, 31, 63, 127]  # grid inicial\n",
        ")\n",
        "\n",
        "print(f\"\\n>> Mejor num_leaves en barrido grueso: {best_leaves_coarse}\")\n",
        "\n",
        "# 2) Barrido fino alrededor del mejor num_leaves\n",
        "refined_grid = sorted(\n",
        "    set(\n",
        "        max(2, best_leaves_coarse + delta)\n",
        "        for delta in [-24, -16, -8, 0, 8, 16, 24]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\n==== VALIDACIÓN RÁPIDA CON LGBM + TUNING FINO alrededor de {best_leaves_coarse} ====\")\n",
        "print(\"Grid fino:\", refined_grid)\n",
        "\n",
        "model_val_fine, best_leaves_fine = train_and_evaluate_lgbm(\n",
        "    X, y, cat_cols,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    leaves_grid=refined_grid\n",
        ")\n",
        "\n",
        "best_iter = model_val_fine.best_iteration\n",
        "print(f\"\\n>> Mejor num_leaves tras barrido fino: {best_leaves_fine}\")\n",
        "print(f\">> Mejor iteración final: {best_iter}\")\n",
        "\n",
        "print(\"\\n==== ENTRENAMIENTO FINAL Y SUBMISSION (con num_leaves fino) ====\")\n",
        "submission = train_full_and_predict_lgbm(\n",
        "    X, y, X_test,\n",
        "    cat_cols,\n",
        "    sample_sub,\n",
        "    best_iteration=best_iter,\n",
        "    best_num_leaves=best_leaves_fine\n",
        ")\n",
        "\n",
        "out_path = DATA_DIR / \"submission_lgbm_agg_leaves_tuned_fine.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(f\"\\n✅ Submission guardada en:\\n{out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGJtSoFbAxTl",
        "outputId": "9ecd1016-9f24-47ca-9ee5-42495dcf65da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Leyendo CSV...\n",
            "Renombrando columnas Unnamed en test.csv:\n",
            "{'Unnamed: 28': 'year', 'Unnamed: 29': 'num_week_iso', 'Unnamed: 30': 'weekly_sales', 'Unnamed: 31': 'weekly_demand', 'Unnamed: 32': 'Production'}\n",
            "train shape: (95339, 33)\n",
            "test shape: (2250, 33)\n",
            "sample_submission shape: (2250, 2)\n",
            "Número de features: 32\n",
            "Categóricas: 18\n",
            "\n",
            "==== VALIDACIÓN RÁPIDA CON LGBM + TUNING GRUESO num_leaves ====\n",
            "\n",
            "[VALIDACIÓN] Haciendo train/valid split...\n",
            "X_train: (7874, 32)  X_valid: (1969, 32)\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 15 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3893]\ttrain's rmse: 3940.89\tvalid's rmse: 6675.87\n",
            "[RESULTADO] num_leaves=15 -> RMSE valid: 6675.8738, best_iter=3893\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 31 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4249]\ttrain's rmse: 2719.59\tvalid's rmse: 6629.2\n",
            "[RESULTADO] num_leaves=31 -> RMSE valid: 6629.1959, best_iter=4249\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 63 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4237]\ttrain's rmse: 2053.67\tvalid's rmse: 6602.79\n",
            "[RESULTADO] num_leaves=63 -> RMSE valid: 6602.7898, best_iter=4237\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 127 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3773]\ttrain's rmse: 1979.2\tvalid's rmse: 6662.87\n",
            "[RESULTADO] num_leaves=127 -> RMSE valid: 6662.8748, best_iter=3773\n",
            "\n",
            "==============================\n",
            "MEJOR num_leaves: 63 con RMSE: 6602.7898\n",
            "Mejor iteración: 4237\n",
            "==============================\n",
            "\n",
            ">> Mejor num_leaves en barrido grueso: 63\n",
            "\n",
            "==== VALIDACIÓN RÁPIDA CON LGBM + TUNING FINO alrededor de 63 ====\n",
            "Grid fino: [39, 47, 55, 63, 71, 79, 87]\n",
            "\n",
            "[VALIDACIÓN] Haciendo train/valid split...\n",
            "X_train: (7874, 32)  X_valid: (1969, 32)\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 39 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5170]\ttrain's rmse: 2110.48\tvalid's rmse: 6608.85\n",
            "[RESULTADO] num_leaves=39 -> RMSE valid: 6608.8509, best_iter=5170\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 47 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4779]\ttrain's rmse: 2067.56\tvalid's rmse: 6613.23\n",
            "[RESULTADO] num_leaves=47 -> RMSE valid: 6613.2336, best_iter=4779\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 55 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4240]\ttrain's rmse: 2151.36\tvalid's rmse: 6606.62\n",
            "[RESULTADO] num_leaves=55 -> RMSE valid: 6606.6153, best_iter=4240\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 63 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4237]\ttrain's rmse: 2053.67\tvalid's rmse: 6602.79\n",
            "[RESULTADO] num_leaves=63 -> RMSE valid: 6602.7898, best_iter=4237\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 71 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4805]\ttrain's rmse: 1778.92\tvalid's rmse: 6613.82\n",
            "[RESULTADO] num_leaves=71 -> RMSE valid: 6613.8235, best_iter=4805\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 79 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4326]\ttrain's rmse: 1892.74\tvalid's rmse: 6620.98\n",
            "[RESULTADO] num_leaves=79 -> RMSE valid: 6620.9799, best_iter=4326\n",
            "\n",
            "[VALIDACIÓN] Probando num_leaves = 87 ...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3773]\ttrain's rmse: 2092.73\tvalid's rmse: 6641.75\n",
            "[RESULTADO] num_leaves=87 -> RMSE valid: 6641.7503, best_iter=3773\n",
            "\n",
            "==============================\n",
            "MEJOR num_leaves: 63 con RMSE: 6602.7898\n",
            "Mejor iteración: 4237\n",
            "==============================\n",
            "\n",
            ">> Mejor num_leaves tras barrido fino: 63\n",
            ">> Mejor iteración final: 4237\n",
            "\n",
            "==== ENTRENAMIENTO FINAL Y SUBMISSION (con num_leaves fino) ====\n",
            "\n",
            "[FINAL] Entrenando modelo LightGBM con TODO el train...\n",
            "[FINAL] Prediciendo sobre test...\n",
            "\n",
            "✅ Submission guardada en:\n",
            "/content/drive/MyDrive/Datathon/submission_lgbm_agg_leaves_tuned_fine.csv\n"
          ]
        }
      ]
    }
  ]
}