{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"Yyzxcjn1UENL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763251746349,"user_tz":-60,"elapsed":2385,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"65c88177-f880-4bbe-ce66-7bf09c6f67f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","!pip install catboost"],"metadata":{"id":"m_pqa0Hwbfom","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763251767538,"user_tz":-60,"elapsed":21187,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"29af281c-dad4-44dd-f562-a6d926980246"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"]}]},{"cell_type":"code","source":["# train = pd.read_csv(\"/content/drive/MyDrive/mango/train.csv\")\n","# test = pd.read_csv(\"/content/drive/MyDrive/mango/test.csv\")\n","# sample = pd.read_csv(\"/content/drive/MyDrive/mango/sample_submission.csv\")\n","\n","df_train = pd.read_csv('/content/drive/MyDrive/Datathon/train.csv', sep=';')\n","df_test = pd.read_csv('/content/drive/MyDrive/Datathon/test.csv', sep=';')"],"metadata":{"id":"DYdJ5cULbl5u","executionInfo":{"status":"ok","timestamp":1763251791234,"user_tz":-60,"elapsed":23683,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["df_train.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0K04rGcgwaR","executionInfo":{"status":"ok","timestamp":1763251791527,"user_tz":-60,"elapsed":279,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"4aae4281-6965-4bee-fc7f-2c762ec1a7c7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 95339 entries, 0 to 95338\n","Data columns (total 33 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   ID                  95339 non-null  int64  \n"," 1   id_season           95339 non-null  int64  \n"," 2   aggregated_family   95339 non-null  object \n"," 3   family              95339 non-null  object \n"," 4   category            95339 non-null  object \n"," 5   fabric              95339 non-null  object \n"," 6   color_name          95339 non-null  object \n"," 7   color_rgb           95339 non-null  object \n"," 8   image_embedding     95339 non-null  object \n"," 9   length_type         86830 non-null  object \n"," 10  silhouette_type     82972 non-null  object \n"," 11  waist_type          23252 non-null  object \n"," 12  neck_lapel_type     58874 non-null  object \n"," 13  sleeve_length_type  57336 non-null  object \n"," 14  heel_shape_type     0 non-null      float64\n"," 15  toecap_type         0 non-null      float64\n"," 16  woven_structure     62510 non-null  object \n"," 17  knit_structure      16926 non-null  object \n"," 18  print_type          95239 non-null  object \n"," 19  archetype           55607 non-null  object \n"," 20  moment              95339 non-null  object \n"," 21  phase_in            95339 non-null  object \n"," 22  phase_out           95339 non-null  object \n"," 23  life_cycle_length   95339 non-null  int64  \n"," 24  num_stores          95339 non-null  int64  \n"," 25  num_sizes           95339 non-null  int64  \n"," 26  has_plus_sizes      95339 non-null  bool   \n"," 27  price               95339 non-null  float64\n"," 28  year                95339 non-null  int64  \n"," 29  num_week_iso        95339 non-null  int64  \n"," 30  weekly_sales        95339 non-null  int64  \n"," 31  weekly_demand       95339 non-null  int64  \n"," 32  Production          95339 non-null  int64  \n","dtypes: bool(1), float64(3), int64(10), object(19)\n","memory usage: 23.4+ MB\n"]}]},{"cell_type":"code","source":["# Cuenta los valores nulos por columna y los ordena de mayor a menor\n","missing_values = df_train.isnull().sum().sort_values(ascending=False)\n","\n","# Filtra solo las columnas que tienen al menos un valor nulo\n","#missing_values = missing_values[missing_values > 0]\n","\n","print(missing_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNjoaGeThxKv","executionInfo":{"status":"ok","timestamp":1763251791815,"user_tz":-60,"elapsed":286,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"a6c32091-89b5-4b4b-8066-abd95fea57cc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["heel_shape_type       95339\n","toecap_type           95339\n","knit_structure        78413\n","waist_type            72087\n","archetype             39732\n","sleeve_length_type    38003\n","neck_lapel_type       36465\n","woven_structure       32829\n","silhouette_type       12367\n","length_type            8509\n","print_type              100\n","ID                        0\n","image_embedding           0\n","category                  0\n","color_rgb                 0\n","color_name                0\n","fabric                    0\n","id_season                 0\n","aggregated_family         0\n","family                    0\n","moment                    0\n","phase_in                  0\n","phase_out                 0\n","life_cycle_length         0\n","num_stores                0\n","num_sizes                 0\n","has_plus_sizes            0\n","price                     0\n","year                      0\n","num_week_iso              0\n","weekly_sales              0\n","weekly_demand             0\n","Production                0\n","dtype: int64\n"]}]},{"cell_type":"code","source":["COLUMNAS_VACIAS = [\n","    'heel_shape_type',\n","    'toecap_type'\n","    # Si también quieres eliminar las columnas \"Unnamed\", añádelas aquí:\n","    # 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32'\n","]\n","df_train = df_train.drop(columns=COLUMNAS_VACIAS, errors='ignore')\n","df_test = df_test.drop(columns=COLUMNAS_VACIAS, errors='ignore')"],"metadata":{"id":"NVLmVIxciYHP","executionInfo":{"status":"ok","timestamp":1763251791819,"user_tz":-60,"elapsed":2,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["object_cols = df_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n","id_cols = ['ID', 'id_season']\n","\n","CATEGORICAL_FEATURES = list(set(object_cols + id_cols))\n","\n","for col in CATEGORICAL_FEATURES:\n","    # Esto asegura que apliques fillna() a las columnas correctas\n","    if col in df_train.columns:\n","        df_train[col] = df_train[col].fillna('MISSING')\n","    if col in df_test.columns:\n","        df_test[col] = df_test[col].fillna('MISSING')"],"metadata":{"id":"quSzsQ_cF3nA","executionInfo":{"status":"ok","timestamp":1763251792236,"user_tz":-60,"elapsed":416,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Columnas que sabes que son basura y solo aparecen en el test set\n","TEST_EXCLUSIVOS = ['Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32']\n","\n","# Eliminar columnas basura del test set\n","df_test = df_test.drop(columns=TEST_EXCLUSIVOS, errors='ignore')"],"metadata":{"id":"5HrlMCqhI6PM","executionInfo":{"status":"ok","timestamp":1763251792256,"user_tz":-60,"elapsed":10,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print(df_train.columns)\n","print(\"\")\n","print(df_test.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kBa0G8mGgXQ","executionInfo":{"status":"ok","timestamp":1763251792284,"user_tz":-60,"elapsed":19,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"c7ddc3cd-5f98-4861-a890-bded9e01853a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price', 'year',\n","       'num_week_iso', 'weekly_sales', 'weekly_demand', 'Production'],\n","      dtype='object')\n","\n","Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# --- 1. AGREGACIÓN DE RENDIMIENTO HISTÓRICO (SOLO EN df_TRAIN) ---\n","\n","# Agrupar por ID y calcular estadísticas clave\n","historical_features = df_train.groupby('ID').agg(\n","    # Medias de rendimiento (potencial de venta)\n","    hist_media_demand=pd.NamedAgg(column='weekly_demand', aggfunc='mean'),\n","    hist_media_sales=pd.NamedAgg(column='weekly_sales', aggfunc='mean'),\n","\n","    # Desviación Estándar (volatilidad/riesgo)\n","    hist_std_sales=pd.NamedAgg(column='weekly_sales', aggfunc='std'),\n","\n","    # Máximo de Demanda (pico de éxito)\n","    hist_max_demand=pd.NamedAgg(column='weekly_demand', aggfunc='max'),\n","\n","    # Conteo (longevidad) - Usamos una columna disponible para contar\n","    hist_num_weeks_active=pd.NamedAgg(column='weekly_demand', aggfunc='count')\n",").reset_index() # .reset_index() convierte ID de vuelta a columna\n","\n","# Rellenar los NaN creados por la desviación estándar con 0\n","historical_features = historical_features.fillna(0)\n","\n","print(historical_features.head())\n","\n","# Unir estas nuevas features estáticas a ambos DataFrames\n","# Nota: Los IDs nuevos en df_test que no estén en df_train recibirán NaN aquí.\n","df_train = pd.merge(df_train, historical_features, on='ID', how='left')\n","df_test = pd.merge(df_test, historical_features, on='ID', how='left')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWXCu4hgOcOv","executionInfo":{"status":"ok","timestamp":1763251792627,"user_tz":-60,"elapsed":336,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"fbb92da8-ec30-4bae-8ab6-3b24e71a9038"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["   ID  hist_media_demand  hist_media_sales  hist_std_sales  hist_max_demand  \\\n","0   1          67.166667         66.833333       32.663111              135   \n","1   2         188.833333        182.083333       47.419709              259   \n","2   3        3543.944444       3081.777778     1071.132911             5441   \n","3   4        1375.500000       1257.750000      947.717823             2672   \n","4   6        1835.500000       1717.750000      385.623706             2303   \n","\n","   hist_num_weeks_active  \n","0                     12  \n","1                     12  \n","2                     18  \n","3                      8  \n","4                      8  \n"]}]},{"cell_type":"code","source":["# --- 2. DESCOMPOSICIÓN DE FECHAS (phase_in, phase_out) ---\n","\n","def extract_date_features(df):\n","    # Asegurarse de que la columna sea de tipo string antes de fillna (por si hay nulos)\n","    df['phase_in'] = df['phase_in'].astype(str).fillna('1900-01-01')\n","\n","    # Convertir a datetime\n","    df['phase_in_dt'] = pd.to_datetime(df['phase_in'], errors='coerce')\n","\n","    # Extraer features clave de estacionalidad\n","    df['launch_month'] = df['phase_in_dt'].dt.month\n","    df['launch_quarter'] = df['phase_in_dt'].dt.quarter\n","    df['launch_weekday'] = df['phase_in_dt'].dt.dayofweek\n","\n","    # Limpiar columnas temporales intermedias (opcional)\n","    df = df.drop(columns=['phase_in_dt'], errors='ignore')\n","\n","    return df\n","\n","df_train = extract_date_features(df_train)\n","df_test = extract_date_features(df_test)\n","\n","print(\"✅ Features de fechas (launch_month, etc.) extraídas en ambos sets.\")"],"metadata":{"id":"GN3Rlc3nJSZ-","executionInfo":{"status":"ok","timestamp":1763251792914,"user_tz":-60,"elapsed":292,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa0b5f25-7a7d-427d-e893-501afed39361"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Features de fechas (launch_month, etc.) extraídas en ambos sets.\n"]}]},{"cell_type":"code","source":["print(df_train.columns)\n","print(\"\")\n","print(df_test.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRxsOYXvQKEW","executionInfo":{"status":"ok","timestamp":1763251792951,"user_tz":-60,"elapsed":34,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"212735b3-ebad-4e6e-f88c-2e3c0cb40a92"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price', 'year',\n","       'num_week_iso', 'weekly_sales', 'weekly_demand', 'Production',\n","       'hist_media_demand', 'hist_media_sales', 'hist_std_sales',\n","       'hist_max_demand', 'hist_num_weeks_active', 'launch_month',\n","       'launch_quarter', 'launch_weekday'],\n","      dtype='object')\n","\n","Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price',\n","       'hist_media_demand', 'hist_media_sales', 'hist_std_sales',\n","       'hist_max_demand', 'hist_num_weeks_active', 'launch_month',\n","       'launch_quarter', 'launch_weekday'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["# 1. Definir la variable Target (y)\n","df_train_y = df_train['Production']\n","\n","# 2. Columnas a excluir de las features (X)\n","# Excluimos el Target y las variables temporales/históricas que ya hemos transformado o no necesitamos\n","EXCLUDE_COLS = [\n","    'weekly_sales', 'weekly_demand', 'Production', 'year', 'num_week_iso'\n","]\n","\n","# 3. Definir las Features (X)\n","df_train_x = df_train.drop(columns=EXCLUDE_COLS, errors='ignore')\n","df_test_x = df_test.drop(columns=EXCLUDE_COLS, errors='ignore')"],"metadata":{"id":"KJmcLkhHQIyO","executionInfo":{"status":"ok","timestamp":1763251793040,"user_tz":-60,"elapsed":81,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(df_train_x.columns)\n","print(\"\")\n","print(df_test_x.columns)\n","print(\"\")\n","print(df_test_x.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOHs2PdDSRdy","executionInfo":{"status":"ok","timestamp":1763251793103,"user_tz":-60,"elapsed":57,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"b1121214-e924-4c1c-f94e-39ae5219fe09"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price',\n","       'hist_media_demand', 'hist_media_sales', 'hist_std_sales',\n","       'hist_max_demand', 'hist_num_weeks_active', 'launch_month',\n","       'launch_quarter', 'launch_weekday'],\n","      dtype='object')\n","\n","Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'image_embedding', 'length_type',\n","       'silhouette_type', 'waist_type', 'neck_lapel_type',\n","       'sleeve_length_type', 'woven_structure', 'knit_structure', 'print_type',\n","       'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length',\n","       'num_stores', 'num_sizes', 'has_plus_sizes', 'price',\n","       'hist_media_demand', 'hist_media_sales', 'hist_std_sales',\n","       'hist_max_demand', 'hist_num_weeks_active', 'launch_month',\n","       'launch_quarter', 'launch_weekday'],\n","      dtype='object')\n","\n","    ID  id_season       aggregated_family     family  \\\n","0   90         90       Skirts and shorts     Shorts   \n","1   16         90     Jackets and Blazers    Jackets   \n","2   65         90  Sweaters and Cardigans  Cardigans   \n","3  138         90   Dresses and jumpsuits    Dresses   \n","4  166         90       Swim and intimate   Swimwear   \n","\n","                              category    fabric color_name    color_rgb  \\\n","0                              Bottoms     WOVEN     BLANCO  245,244,238   \n","1                            Outerwear     WOVEN     MARRON   129,105,81   \n","2                                 Tops  CIRCULAR       GRIS  145,145,144   \n","3  Dresses, jumpsuits and Complete set     WOVEN      ARENA  219,207,189   \n","4        Accesories, Swim and Intimate  CIRCULAR       ROJO    189,22,62   \n","\n","                                     image_embedding length_type  ...  \\\n","0  -0.12171847,-0.25323686,0.6839907,-0.60686946,...       Short  ...   \n","1  -0.2704233,-1.1392772,1.2337782,-0.64782524,0....       Short  ...   \n","2  -0.16349873,0.10306308,1.1780332,-0.6578121,0....    Standard  ...   \n","3  0.13487932,0.004863858,-0.52593106,-0.8941919,...        Long  ...   \n","4  0.08456349,0.06271024,-0.11037013,-0.6067481,0...     MISSING  ...   \n","\n","  has_plus_sizes  price hist_media_demand hist_media_sales hist_std_sales  \\\n","0          False  29.99               NaN              NaN            NaN   \n","1          False  69.99               NaN              NaN            NaN   \n","2          False  22.99               NaN              NaN            NaN   \n","3          False  89.99            1035.1           896.75     636.653494   \n","4          False  17.99             941.4           902.70     754.324429   \n","\n","  hist_max_demand hist_num_weeks_active launch_month launch_quarter  \\\n","0             NaN                   NaN         10.0            4.0   \n","1             NaN                   NaN          9.0            3.0   \n","2             NaN                   NaN          NaN            NaN   \n","3          2315.0                  20.0         10.0            4.0   \n","4          2001.0                  10.0          3.0            1.0   \n","\n","  launch_weekday  \n","0            3.0  \n","1            3.0  \n","2            NaN  \n","3            3.0  \n","4            0.0  \n","\n","[5 rows x 34 columns]\n"]}]},{"cell_type":"code","source":["# Bloque de Imputación Final para las Features Numéricas\n","\n","NUMERIC_FEATURES = df_train_x.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","\n","for col in NUMERIC_FEATURES:\n","    # Si la columna existe en df_test, se imputan los NaN con 0\n","    if col in df_train_x.columns:\n","        df_train_x[col] = df_train_x[col].fillna(0)\n","    if col in df_test_x.columns:\n","        df_test_x[col] = df_test_x[col].fillna(0)"],"metadata":{"id":"Iy36wOLQThJF","executionInfo":{"status":"ok","timestamp":1763251793174,"user_tz":-60,"elapsed":74,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Cuenta los valores nulos por columna y los ordena de mayor a menor\n","missing_values = df_test_x.isnull().sum().sort_values(ascending=False)\n","\n","# Filtra solo las columnas que tienen al menos un valor nulo\n","#missing_values = missing_values[missing_values > 0]\n","\n","print(missing_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR7mvq9qT28z","executionInfo":{"status":"ok","timestamp":1763251793239,"user_tz":-60,"elapsed":57,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"6e32cb38-6ef5-4277-ade6-a3dbf4763420"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["ID                       0\n","id_season                0\n","aggregated_family        0\n","family                   0\n","category                 0\n","fabric                   0\n","color_name               0\n","color_rgb                0\n","image_embedding          0\n","length_type              0\n","silhouette_type          0\n","waist_type               0\n","neck_lapel_type          0\n","sleeve_length_type       0\n","woven_structure          0\n","knit_structure           0\n","print_type               0\n","archetype                0\n","moment                   0\n","phase_in                 0\n","phase_out                0\n","life_cycle_length        0\n","num_stores               0\n","num_sizes                0\n","has_plus_sizes           0\n","price                    0\n","hist_media_demand        0\n","hist_media_sales         0\n","hist_std_sales           0\n","hist_max_demand          0\n","hist_num_weeks_active    0\n","launch_month             0\n","launch_quarter           0\n","launch_weekday           0\n","dtype: int64\n"]}]},{"cell_type":"code","source":["from catboost import Pool\n","\n","# =========================================================\n","# 1. Definición de la Lista de Features Categóricas Finales\n","# =========================================================\n","\n","# Columnas de tipo texto/booleano\n","CATEGORICAL_FEATURES = df_train_x.select_dtypes(include=['object', 'bool']).columns.tolist()\n","\n","# Añadir IDs y columnas de fecha/tiempo que deben ser tratadas como categorías\n","ID_AND_TIME_COLS = ['ID', 'id_season', 'launch_month', 'launch_quarter', 'launch_weekday', 'life_cycle_length']\n","\n","# Aseguramos que solo incluimos las columnas que existen y no duplicamos\n","CATEGORICAL_FEATURES.extend([col for col in ID_AND_TIME_COLS if col in df_train_x.columns and col not in CATEGORICAL_FEATURES])\n"],"metadata":{"id":"mLgTmxoqWAMo","executionInfo":{"status":"ok","timestamp":1763251797744,"user_tz":-60,"elapsed":4505,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Columnas que son categorías pero que Pandas pudo haber dejado como float (ej. 2.0, 1.0)\n","NUMERICAL_CATEGORICAL_COLS = ['ID', 'id_season', 'launch_month', 'launch_quarter', 'launch_weekday', 'life_cycle_length']\n","\n","print(\"Corrigiendo tipos de dato: convirtiendo categorías numéricas a string...\")\n","\n","for col in NUMERICAL_CATEGORICAL_COLS:\n","    if col in df_train_x.columns:\n","        # Forzar la conversión a string. Esto convierte 2.0 a \"2.0\".\n","        df_train_x[col] = df_train_x[col].astype(str)\n","\n","# Si df_test_x existe, aplica la misma corrección\n","if 'df_test_x' in locals():\n","    for col in NUMERICAL_CATEGORICAL_COLS:\n","        if col in df_test_x.columns:\n","            df_test_x[col] = df_test_x[col].astype(str)\n","\n","print(\"✅ Tipos de dato corregidos. Vuelve a ejecutar la creación del Pool.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIWiUslnYHjp","executionInfo":{"status":"ok","timestamp":1763251798559,"user_tz":-60,"elapsed":807,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"afb4571e-aabd-46d3-d658-6a8c61ccaa8f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Corrigiendo tipos de dato: convirtiendo categorías numéricas a string...\n","✅ Tipos de dato corregidos. Vuelve a ejecutar la creación del Pool.\n"]}]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","\n","# =========================================================\n","# PASO 1: PARSEAR EL EMBEDDING DE STRING A MATRIZ NUMÉRICA\n","# =========================================================\n","\n","def parse_embedding(df, column_name='image_embedding'):\n","    \"\"\"Convierte la columna de embedding (string) en un DataFrame de floats.\"\"\"\n","\n","    # 1. Asegurar que la columna es string y rellenar NaN si existen\n","    # (Aunque no deberían si ya limpiaste categóricas, es un paso de seguridad)\n","    df[column_name] = df[column_name].astype(str).str.replace('[', '').str.replace(']', '')\n","\n","    # 2. Dividir la string por coma y crear un nuevo DataFrame\n","    embedding_df = df[column_name].str.split(',', expand=True).astype(float)\n","\n","    # 3. Asignar nombres a las columnas (ej. embed_0, embed_1, ...)\n","    num_components = embedding_df.shape[1]\n","    embedding_df.columns = [f'embed_{i}' for i in range(num_components)]\n","\n","    # 4. Reemplazar NaN (si la string estaba vacía) con 0\n","    embedding_df = embedding_df.fillna(0)\n","\n","    return embedding_df\n","\n","# Obtener la matriz numérica de embeddings solo para el set de entrenamiento\n","X_embed_train = parse_embedding(df_train_x.copy(), 'image_embedding')\n","X_embed_test = parse_embedding(df_test_x.copy(), 'image_embedding') # Necesario para aplicar el mismo PCA después\n","\n","print(f\"Dimensiones de la matriz de embeddings: {X_embed_train.shape}\")\n","# Ejemplo de output: Dimensiones de la matriz de embeddings: (10000, 50) si tienes 50 dimensiones"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eRzJ9wAoQDq","executionInfo":{"status":"ok","timestamp":1763251904147,"user_tz":-60,"elapsed":105586,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"7416d4cf-9101-40b6-b4c8-8b0b7c4b41a9"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensiones de la matriz de embeddings: (95339, 512)\n"]}]},{"cell_type":"code","source":["# =========================================================\n","# PASO 2: ESTANDARIZAR LOS DATOS (CRUCIAL PARA PCA)\n","# =========================================================\n","\n","# Inicializar y ajustar el escalador SÓLO en el set de entrenamiento\n","scaler = StandardScaler()\n","X_embed_train_scaled = scaler.fit_transform(X_embed_train)\n","X_embed_test_scaled = scaler.transform(X_embed_test) # Aplicar el mismo escalado al test\n"],"metadata":{"id":"itvvF1mnqb-J","executionInfo":{"status":"ok","timestamp":1763251905160,"user_tz":-60,"elapsed":1018,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# =========================================================\n","# PASO 3: APLICAR PCA\n","# =========================================================\n","\n","# Inicializamos PCA sin límite (None) para analizar toda la varianza\n","pca = PCA(n_components=None, random_state=42)\n","pca.fit(X_embed_train_scaled)\n","\n","# Ahora podemos analizar y decidir K"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"VGlY37R-qf75","executionInfo":{"status":"ok","timestamp":1763251906395,"user_tz":-60,"elapsed":1234,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"ebba1e9c-3788-4433-bec1-1bce2a69b377"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PCA(random_state=42)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>PCA</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(random_state=42)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# =========================================================\n","# PASO 4: CÁLCULO DE LA VARIANZA EXPLICADA ACUMULADA\n","# =========================================================\n","\n","# Varianza explicada por cada componente\n","explained_variance_ratio = pca.explained_variance_ratio_\n","\n","# Varianza explicada acumulada\n","cumulative_variance = np.cumsum(explained_variance_ratio)\n","\n","# Creamos un DataFrame para visualizar\n","variance_df = pd.DataFrame({\n","    'Componente': range(1, len(cumulative_variance) + 1),\n","    'Varianza Acumulada': cumulative_variance\n","})\n","\n","# Mostrar los puntos clave (ej. dónde se alcanza el 80%, 90% y 95%)\n","print(\"\\n--- Varianza Explicada Acumulada ---\")\n","for percentage in [0.80, 0.90, 0.95]:\n","    # Encontrar el índice de la primera componente que supera el umbral\n","    num_components = np.argmax(cumulative_variance >= percentage) + 1\n","    print(f\"Para el {int(percentage*100)}% de varianza, se necesitan {num_components} componentes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rp_64OEpqRlV","executionInfo":{"status":"ok","timestamp":1763251906406,"user_tz":-60,"elapsed":7,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"d3dda0bc-7909-4a3f-d1b3-a1a41543c89d"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Varianza Explicada Acumulada ---\n","Para el 80% de varianza, se necesitan 61 componentes.\n","Para el 90% de varianza, se necesitan 119 componentes.\n","Para el 95% de varianza, se necesitan 193 componentes.\n"]}]},{"cell_type":"code","source":["# Definición del número óptimo de componentes\n","K_OPTIMO = 119\n","\n","# 1. Re-inicializar PCA con K=119\n","pca_final = PCA(n_components=K_OPTIMO, random_state=42)\n","\n","# 2. Ajustar y transformar el set de entrenamiento\n","# Nota: X_embed_train_scaled debe ser la matriz estandarizada\n","X_pca_train = pca_final.fit_transform(X_embed_train_scaled)\n","\n","# 3. Transformar el set de prueba usando el mismo ajuste del entrenamiento\n","X_pca_test = pca_final.transform(X_embed_test_scaled)\n","\n","# 4. Crear los DataFrames de Componentes\n","componentes_retenidas = [f'PC{i}' for i in range(1, K_OPTIMO + 1)]\n","\n","df_pca_train = pd.DataFrame(data=X_pca_train, columns=componentes_retenidas, index=df_train_x.index)\n","df_pca_test = pd.DataFrame(data=X_pca_test, columns=componentes_retenidas, index=df_test_x.index)"],"metadata":{"id":"xjIxJzBDt5v0","executionInfo":{"status":"ok","timestamp":1763251909053,"user_tz":-60,"elapsed":2645,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 1. Eliminar la columna de embedding original del set de entrenamiento\n","df_train_x = df_train_x.drop(columns=['image_embedding'], errors='ignore')\n","\n","# 2. Eliminar la columna de embedding original del set de prueba\n","df_test_x = df_test_x.drop(columns=['image_embedding'], errors='ignore')\n","\n","print(\"Columna 'image_embedding' original eliminada de ambos DataFrames.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shZu7vyEuTy6","executionInfo":{"status":"ok","timestamp":1763251909159,"user_tz":-60,"elapsed":103,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"8fe2f26a-2b42-4687-df6b-599e71cf14a9"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Columna 'image_embedding' original eliminada de ambos DataFrames.\n"]}]},{"cell_type":"code","source":["# Unir las 119 componentes principales al DataFrame de entrenamiento\n","df_train_x = pd.concat([df_train_x, df_pca_train], axis=1)\n","\n","# Unir las 119 componentes principales al DataFrame de prueba\n","df_test_x = pd.concat([df_test_x, df_pca_test], axis=1)\n","\n","print(f\"✅ Integración PCA completa.\")\n","print(f\"Dimensiones de df_train_x después de la unión: {df_train_x.shape}\")\n","print(f\"Dimensiones de df_test_x después de la unión: {df_test_x.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y21zw15duqcv","executionInfo":{"status":"ok","timestamp":1763251911024,"user_tz":-60,"elapsed":1857,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"1443eea9-ec6f-4eee-bba1-330f30f814ba"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Integración PCA completa.\n","Dimensiones de df_train_x después de la unión: (95339, 152)\n","Dimensiones de df_test_x después de la unión: (2250, 152)\n"]}]},{"cell_type":"code","source":["print(df_train_x.columns)\n","print(\"\")\n","print(df_test_x.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_liM8ZjuZr7","executionInfo":{"status":"ok","timestamp":1763251911047,"user_tz":-60,"elapsed":20,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"36528381-6093-4762-b79c-01e66ae9cbdc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'length_type', 'silhouette_type',\n","       ...\n","       'PC110', 'PC111', 'PC112', 'PC113', 'PC114', 'PC115', 'PC116', 'PC117',\n","       'PC118', 'PC119'],\n","      dtype='object', length=152)\n","\n","Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'length_type', 'silhouette_type',\n","       ...\n","       'PC110', 'PC111', 'PC112', 'PC113', 'PC114', 'PC115', 'PC116', 'PC117',\n","       'PC118', 'PC119'],\n","      dtype='object', length=152)\n"]}]},{"cell_type":"code","source":["!pip install kmodes\n","from kmodes.kprototypes import KPrototypes\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","import warnings\n","\n","warnings.filterwarnings('ignore') # Ocultar advertencias de NumPy/Pandas/Sklearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fx-3dHPs0x8x","executionInfo":{"status":"ok","timestamp":1763251918780,"user_tz":-60,"elapsed":7747,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"48c2d7bf-d23d-4baa-bcf3-4994f78c11cc"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kmodes in /usr/local/lib/python3.12/dist-packages (0.12.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.12/dist-packages (from kmodes) (2.0.2)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from kmodes) (1.6.1)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.12/dist-packages (from kmodes) (1.16.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from kmodes) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->kmodes) (3.6.0)\n"]}]},{"cell_type":"code","source":["# --- 1. Definición de Features para el Clustering ---\n","\n","# Suponemos que tienes de PC1 a PC119 en df_train_x\n","pc_cols = [col for col in df_train_x.columns if col.startswith('PC')]\n","\n","# Features Numéricas (PCA, Precio, Distribución)\n","NUM_FEATURES = pc_cols + ['price', 'life_cycle_length', 'num_stores', 'num_sizes']\n","\n","# Features Categóricas (Diseño y Funcionalidad)\n","CAT_FEATURES = [\n","    'aggregated_family', 'family', 'category', 'fabric', 'color_name',\n","    'silhouette_type', 'length_type', 'waist_type', 'sleeve_length_type',\n","    'archetype', 'moment'\n","]\n","\n","# --- 2. Crear una Matriz Limpia para el Clustering ---\n","\n","# Seleccionamos las features de train (la misma lógica se aplicaría a test)\n","X_train_cluster = df_train_x[NUM_FEATURES + CAT_FEATURES].copy()\n","\n","# Rellenar NaNs en categóricas para asegurar consistencia\n","for col in CAT_FEATURES:\n","    X_train_cluster[col] = X_train_cluster[col].fillna('MISSING_CAT').astype(str)\n","\n","# --- 3. Estandarización de Features Numéricas ---\n","\n","# Se ajusta el escalador SÓLO con los datos de entrenamiento\n","scaler_cluster = StandardScaler()\n","X_train_cluster[NUM_FEATURES] = scaler_cluster.fit_transform(X_train_cluster[NUM_FEATURES])\n","\n","print(f\"Features numéricas escaladas: {len(NUM_FEATURES)}\")\n","print(f\"Features categóricas: {len(CAT_FEATURES)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFnqGX-Q3Ifs","executionInfo":{"status":"ok","timestamp":1763251928818,"user_tz":-60,"elapsed":10036,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"118fbc0d-8824-42ab-ca8e-b920422e176d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Features numéricas escaladas: 123\n","Features categóricas: 11\n"]}]},{"cell_type":"code","source":["# --- 4. Preparar Data Array y Localización de Categorías ---\n","\n","# K-Prototypes necesita un array de numpy\n","X_train_array = X_train_cluster.values\n","\n","# Obtener los índices de las columnas categóricas en el array (muy importante!)\n","cat_cols_index = [X_train_cluster.columns.get_loc(col) for col in CAT_FEATURES]\n","\n","# --- 5. Inicializar y Entrenar K-Prototypes ---\n","\n","K_CLUSTERS = 50 # Valor inicial (debe ser optimizado)\n","SEED = 42\n","\n","print(f\"\\nIniciando K-Prototypes con K={K_CLUSTERS}...\")\n","\n","kproto = KPrototypes(n_clusters=K_CLUSTERS, init='Huang', n_init=5, max_iter=20, random_state=SEED, verbose=2)\n","clusters_train = kproto.fit_predict(X_train_array, categorical=cat_cols_index)\n","\n","# Guardar las etiquetas de cluster en el DataFrame principal\n","df_train_x['cluster_id'] = clusters_train\n","\n","print(\"✅ Clustering completado. Asignaciones añadidas a df_train_x.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdT9xt4X3wzv","executionInfo":{"status":"ok","timestamp":1763254792266,"user_tz":-60,"elapsed":2863447,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"b5db554c-0a35-415d-f269-04a370cb4b51"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iniciando K-Prototypes con K=50...\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run: 1, iteration: 1/20, moves: 70565, ncost: 10125788.50853053\n","Run: 1, iteration: 2/20, moves: 31414, ncost: 9822575.925649827\n","Run: 1, iteration: 3/20, moves: 11699, ncost: 9738011.723441621\n","Run: 1, iteration: 4/20, moves: 5940, ncost: 9714006.52964412\n","Run: 1, iteration: 5/20, moves: 3409, ncost: 9700243.79016421\n","Run: 1, iteration: 6/20, moves: 2381, ncost: 9690754.727608407\n","Run: 1, iteration: 7/20, moves: 2172, ncost: 9681998.763593791\n","Run: 1, iteration: 8/20, moves: 1635, ncost: 9678046.011100993\n","Run: 1, iteration: 9/20, moves: 1689, ncost: 9667534.845495049\n","Run: 1, iteration: 10/20, moves: 1963, ncost: 9650459.54259754\n","Run: 1, iteration: 11/20, moves: 1166, ncost: 9646210.944595315\n","Run: 1, iteration: 12/20, moves: 1338, ncost: 9642990.8570187\n","Run: 1, iteration: 13/20, moves: 1258, ncost: 9640302.285821734\n","Run: 1, iteration: 14/20, moves: 936, ncost: 9638534.565389767\n","Run: 1, iteration: 15/20, moves: 725, ncost: 9636814.095745804\n","Run: 1, iteration: 16/20, moves: 689, ncost: 9635573.024544358\n","Run: 1, iteration: 17/20, moves: 576, ncost: 9634498.148782093\n","Run: 1, iteration: 18/20, moves: 392, ncost: 9633775.121722976\n","Run: 1, iteration: 19/20, moves: 200, ncost: 9633445.925762242\n","Run: 1, iteration: 20/20, moves: 102, ncost: 9633310.364045765\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run: 2, iteration: 1/20, moves: 70400, ncost: 10065647.76640963\n","Run: 2, iteration: 2/20, moves: 30112, ncost: 9802845.076871693\n","Run: 2, iteration: 3/20, moves: 11423, ncost: 9745931.295088291\n","Run: 2, iteration: 4/20, moves: 5431, ncost: 9726002.599234588\n","Run: 2, iteration: 5/20, moves: 4118, ncost: 9698655.082938824\n","Run: 2, iteration: 6/20, moves: 3055, ncost: 9678746.215224119\n","Run: 2, iteration: 7/20, moves: 2423, ncost: 9668492.61757001\n","Run: 2, iteration: 8/20, moves: 1566, ncost: 9663111.854349572\n","Run: 2, iteration: 9/20, moves: 1234, ncost: 9658325.552086448\n","Run: 2, iteration: 10/20, moves: 894, ncost: 9655017.53412424\n","Run: 2, iteration: 11/20, moves: 652, ncost: 9653239.246527195\n","Run: 2, iteration: 12/20, moves: 604, ncost: 9650217.08028501\n","Run: 2, iteration: 13/20, moves: 404, ncost: 9648687.85200756\n","Run: 2, iteration: 14/20, moves: 360, ncost: 9648033.451236397\n","Run: 2, iteration: 15/20, moves: 186, ncost: 9647788.090663739\n","Run: 2, iteration: 16/20, moves: 138, ncost: 9647567.388382435\n","Run: 2, iteration: 17/20, moves: 214, ncost: 9647001.977396835\n","Run: 2, iteration: 18/20, moves: 176, ncost: 9646760.242598843\n","Run: 2, iteration: 19/20, moves: 104, ncost: 9646569.681615861\n","Run: 2, iteration: 20/20, moves: 26, ncost: 9646543.757427027\n","Init: initializing centroids\n","Init: initializing clusters\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run: 3, iteration: 1/20, moves: 69941, ncost: 10114381.810140882\n","Run: 3, iteration: 2/20, moves: 30862, ncost: 9811508.517950956\n","Run: 3, iteration: 3/20, moves: 11955, ncost: 9742875.670913514\n","Run: 3, iteration: 4/20, moves: 6528, ncost: 9711894.508243334\n","Run: 3, iteration: 5/20, moves: 4508, ncost: 9696313.215958623\n","Run: 3, iteration: 6/20, moves: 2564, ncost: 9688648.970182922\n","Run: 3, iteration: 7/20, moves: 1358, ncost: 9685457.783902735\n","Run: 3, iteration: 8/20, moves: 814, ncost: 9684202.594671844\n","Run: 3, iteration: 9/20, moves: 672, ncost: 9682827.848670423\n","Run: 3, iteration: 10/20, moves: 900, ncost: 9680084.414412258\n","Run: 3, iteration: 11/20, moves: 968, ncost: 9676684.74458149\n","Run: 3, iteration: 12/20, moves: 870, ncost: 9673830.108694548\n","Run: 3, iteration: 13/20, moves: 924, ncost: 9670892.894400505\n","Run: 3, iteration: 14/20, moves: 652, ncost: 9668750.655169798\n","Run: 3, iteration: 15/20, moves: 502, ncost: 9667707.532868816\n","Run: 3, iteration: 16/20, moves: 320, ncost: 9667232.230391707\n","Run: 3, iteration: 17/20, moves: 188, ncost: 9666968.858362462\n","Run: 3, iteration: 18/20, moves: 78, ncost: 9666894.867186878\n","Run: 3, iteration: 19/20, moves: 28, ncost: 9666868.550295789\n","Run: 3, iteration: 20/20, moves: 8, ncost: 9666859.726212574\n","Init: initializing centroids\n","Init: initializing clusters\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run: 4, iteration: 1/20, moves: 68627, ncost: 10220175.245274996\n","Run: 4, iteration: 2/20, moves: 31016, ncost: 9943121.873678355\n","Run: 4, iteration: 3/20, moves: 12189, ncost: 9869268.634918936\n","Run: 4, iteration: 4/20, moves: 7264, ncost: 9819872.8825456\n","Run: 4, iteration: 5/20, moves: 4705, ncost: 9799525.416789478\n","Run: 4, iteration: 6/20, moves: 3028, ncost: 9787390.562923744\n","Run: 4, iteration: 7/20, moves: 1948, ncost: 9781818.423855187\n","Run: 4, iteration: 8/20, moves: 1492, ncost: 9776383.562370885\n","Run: 4, iteration: 9/20, moves: 1374, ncost: 9771940.720539574\n","Run: 4, iteration: 10/20, moves: 1352, ncost: 9765674.847220726\n","Run: 4, iteration: 11/20, moves: 1586, ncost: 9758311.726134146\n","Run: 4, iteration: 12/20, moves: 1974, ncost: 9748182.901552938\n","Run: 4, iteration: 13/20, moves: 2734, ncost: 9730100.751732161\n","Run: 4, iteration: 14/20, moves: 2860, ncost: 9710626.448603284\n","Run: 4, iteration: 15/20, moves: 2323, ncost: 9695313.831635829\n","Run: 4, iteration: 16/20, moves: 1332, ncost: 9689549.65036865\n","Run: 4, iteration: 17/20, moves: 1400, ncost: 9679265.289410904\n","Run: 4, iteration: 18/20, moves: 780, ncost: 9676793.441663591\n","Run: 4, iteration: 19/20, moves: 506, ncost: 9675776.383712944\n","Run: 4, iteration: 20/20, moves: 478, ncost: 9674295.154216234\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run: 5, iteration: 1/20, moves: 69229, ncost: 10098193.640686734\n","Run: 5, iteration: 2/20, moves: 30544, ncost: 9826792.42132788\n","Run: 5, iteration: 3/20, moves: 10604, ncost: 9765209.109316433\n","Run: 5, iteration: 4/20, moves: 6474, ncost: 9735153.729165204\n","Run: 5, iteration: 5/20, moves: 4153, ncost: 9716899.646918876\n","Run: 5, iteration: 6/20, moves: 3306, ncost: 9703911.457172988\n","Run: 5, iteration: 7/20, moves: 2605, ncost: 9688103.314067272\n","Run: 5, iteration: 8/20, moves: 2101, ncost: 9674091.66166686\n","Run: 5, iteration: 9/20, moves: 1144, ncost: 9671488.355699204\n","Run: 5, iteration: 10/20, moves: 556, ncost: 9669993.393884754\n","Run: 5, iteration: 11/20, moves: 496, ncost: 9668785.018509213\n","Run: 5, iteration: 12/20, moves: 862, ncost: 9662392.90906721\n","Run: 5, iteration: 13/20, moves: 1082, ncost: 9658070.656727977\n","Run: 5, iteration: 14/20, moves: 1276, ncost: 9647385.126316736\n","Run: 5, iteration: 15/20, moves: 1103, ncost: 9642839.01533429\n","Run: 5, iteration: 16/20, moves: 612, ncost: 9641302.109178903\n","Run: 5, iteration: 17/20, moves: 388, ncost: 9640559.757764952\n","Run: 5, iteration: 18/20, moves: 118, ncost: 9640408.643116884\n","Run: 5, iteration: 19/20, moves: 58, ncost: 9640332.38604243\n","Run: 5, iteration: 20/20, moves: 32, ncost: 9640302.327896472\n","Best run was number 1\n","✅ Clustering completado. Asignaciones añadidas a df_train_x.\n"]}]},{"cell_type":"code","source":["# --- 6. Calcular Estadísticas del Mercado por Cluster ---\n","\n","# DataFrame temporal para el cálculo de estadísticas (usamos el target original)\n","df_cluster_stats = df_train_x.copy()\n","df_cluster_stats['target'] = df_train_y # Asumimos df_train_y es el target real o log-transformado\n","\n","# Calcular las métricas agregadas\n","cluster_metrics = df_cluster_stats.groupby('cluster_id')['target'].agg(\n","    cluster_mean_demand='mean',\n","    cluster_max_demand='max',\n","    cluster_std_sales='std',\n","    cluster_count='count'\n",").reset_index()\n","\n","# Rellenar NaN en desviación estándar (clusters de tamaño 1)\n","cluster_metrics['cluster_std_sales'] = cluster_metrics['cluster_std_sales'].fillna(0)\n","\n","print(\"\\n--- Estadísticas Agregadas (Top 5 Clusters) ---\")\n","print(cluster_metrics.head())\n","\n","\n","# --- 7. Unir las Features al Set de Entrenamiento y Prueba ---\n","\n","# Unir al set de entrenamiento\n","df_train_x = pd.merge(df_train_x, cluster_metrics, on='cluster_id', how='left')\n","\n","# --- Preparación para Unir al Test ---\n","# NOTA: Para el test, debes:\n","# 1. Escalar X_test_cluster usando scaler_cluster.transform().\n","# 2. Aplicar la asignación de cluster: kproto.predict(X_test_array, categorical=cat_cols_index).\n","# 3. Añadir el cluster_id al df_test_x.\n","# 4. Unir las estadísticas: pd.merge(df_test_x, cluster_metrics, on='cluster_id', how='left').\n","\n","print(\"\\n✅ Features de 'Neighborhood Encoding' añadidas a df_train_x.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jiQGpIpK38Cb","executionInfo":{"status":"ok","timestamp":1763254793115,"user_tz":-60,"elapsed":846,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"903093f9-65c0-45b4-cc37-8d3f6c16ce45"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Estadísticas Agregadas (Top 5 Clusters) ---\n","   cluster_id  cluster_mean_demand  cluster_max_demand  cluster_std_sales  \\\n","0           0         16659.091003              114552       20392.485565   \n","1           1         12422.937644               46695        6988.322534   \n","2           2         21955.786575              110311       22209.327220   \n","3           3         20336.325454              180415       22918.717569   \n","4           4         31267.579932              175377       33678.608929   \n","\n","   cluster_count  \n","0           1934  \n","1            866  \n","2           2324  \n","3           3469  \n","4           2352  \n","\n","✅ Features de 'Neighborhood Encoding' añadidas a df_train_x.\n"]}]},{"cell_type":"code","source":["# 1. Crear la matriz limpia para clustering del set de prueba\n","X_test_cluster = df_test_x[NUM_FEATURES + CAT_FEATURES].copy()\n","\n","# 2. Rellenar NaNs en categóricas (usando el mismo valor)\n","for col in CAT_FEATURES:\n","    X_test_cluster[col] = X_test_cluster[col].fillna('MISSING_CAT').astype(str)\n","\n","# 3. Escalar las Features Numéricas\n","# **IMPORTANTE:** Usar .transform() con el scaler_cluster ajustado en el train.\n","X_test_cluster[NUM_FEATURES] = scaler_cluster.transform(X_test_cluster[NUM_FEATURES])"],"metadata":{"id":"9bTRqwRo4zTY","executionInfo":{"status":"ok","timestamp":1763254793217,"user_tz":-60,"elapsed":101,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# 4. Crear el array de NumPy para la predicción\n","X_test_array = X_test_cluster.values\n","\n","# 5. Predecir los cluster_id del set de prueba\n","# **IMPORTANTE:** Usar .predict() con el modelo kproto ajustado en el train.\n","clusters_test = kproto.predict(X_test_array, categorical=cat_cols_index)\n","\n","# 6. Añadir el cluster_id al DataFrame de prueba\n","df_test_x['cluster_id'] = clusters_test\n","\n","print(\"✅ Cluster IDs asignados al set de prueba.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jC7CPBw8452h","executionInfo":{"status":"ok","timestamp":1763254793490,"user_tz":-60,"elapsed":268,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"75af2bd4-432f-4ec3-b747-560a8ac791ac"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cluster IDs asignados al set de prueba.\n"]}]},{"cell_type":"code","source":["# 7. Unir las estadísticas de rendimiento del cluster (calculadas con datos de entrenamiento)\n","df_test_x = pd.merge(df_test_x, cluster_metrics, on='cluster_id', how='left')\n","\n","# 8. (Opcional pero recomendado) Limpieza final de la columna temporal 'cluster_id'\n","#    Esto evita que CatBoost la confunda con una feature de orden.\n","df_train_x = df_train_x.drop(columns=['cluster_id'], errors='ignore')\n","df_test_x = df_test_x.drop(columns=['cluster_id'], errors='ignore')\n","\n","print(f\"\\n✅ Features de 'Neighborhood Encoding' añadidas a df_test_x.\")\n","print(f\"Dimensiones de df_test_x final: {df_test_x.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlL6zbfZ47k6","executionInfo":{"status":"ok","timestamp":1763254793591,"user_tz":-60,"elapsed":91,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"f590ec75-431b-4a22-d33d-780ef5edc553"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ Features de 'Neighborhood Encoding' añadidas a df_test_x.\n","Dimensiones de df_test_x final: (2250, 156)\n"]}]},{"cell_type":"code","source":["print(df_train_x.columns)\n","print(\"\")\n","print(df_test_x.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-OPaneqcBWJy","executionInfo":{"status":"ok","timestamp":1763255835309,"user_tz":-60,"elapsed":16,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"14d95a71-aaae-44b8-d575-f9ceaf5bb0c9"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'length_type', 'silhouette_type',\n","       ...\n","       'PC118', 'PC119', 'cluster_mean_demand', 'cluster_max_demand',\n","       'cluster_std_sales', 'cluster_count', 'mean_demand_DES',\n","       'max_demand_DES', 'std_sales_DES', 'count_DES'],\n","      dtype='object', length=160)\n","\n","Index(['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric',\n","       'color_name', 'color_rgb', 'length_type', 'silhouette_type',\n","       ...\n","       'PC118', 'PC119', 'cluster_mean_demand', 'cluster_max_demand',\n","       'cluster_std_sales', 'cluster_count', 'mean_demand_DES',\n","       'max_demand_DES', 'std_sales_DES', 'count_DES'],\n","      dtype='object', length=160)\n"]}]},{"cell_type":"code","source":["from kmodes.kmodes import KModes # <-- CAMBIO 1: Importar KModes\n","import pandas as pd\n","import numpy as np\n","\n","# --- 1. Preparación de Features (Solo Categóricas) ---\n","\n","# Features Categóricas (Diseño y Funcionalidad)\n","CAT_FEATURES = [\n","    'aggregated_family', 'family', 'silhouette_type', 'length_type',\n","    'color_name', 'archetype', 'moment'\n","]\n","\n","# Creamos la Matriz con SOLO las Features Categóricas\n","# X_train_cluster debe ser re-creado solo con CAT_FEATURES\n","X_train_cluster = df_train_x[CAT_FEATURES].copy()\n","\n","# Rellenar NaNs y convertir a string (K-Modes trabaja con strings/objetos)\n","for col in CAT_FEATURES:\n","    X_train_cluster[col] = X_train_cluster[col].fillna('MISSING_CAT').astype(str)\n","\n","print(f\"Features categóricas para K-Modes: {len(CAT_FEATURES)}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZXiz1V1sq2i","executionInfo":{"status":"ok","timestamp":1763254793741,"user_tz":-60,"elapsed":118,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"dc1ca658-b33d-44a6-f0f9-ada07b835aaa"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Features categóricas para K-Modes: 7\n"]}]},{"cell_type":"code","source":["# --- 2. Preparar Data Array y Localización de Categorías ---\n","\n","# K-Modes necesita un array de numpy\n","X_train_array = X_train_cluster.values\n","\n","# En K-Modes, todas las columnas son categóricas por defecto.\n","# Si el array solo contiene categóricas, no es necesario pasar 'categorical',\n","# o se puede pasar el índice de todas las columnas: [0, 1, 2, 3, 4, 5, 6]\n","cat_cols_index = list(range(X_train_array.shape[1])) # Índices de todas las columnas\n","\n","\n","# --- 3. Inicializar y Entrenar K-Modes ---\n","\n","K_CLUSTERS = 50\n","SEED = 42\n","NOMBRE_ID_CLUSTER = 'cluster_id_des' # Usar un nombre único para el ID del cluster (ej. 'cluster_id_cat')\n","SUFIJO = '_DES'\n","\n","print(f\"\\nIniciando K-Modes con K={K_CLUSTERS}...\")\n","\n","# CAMBIO 2: Usar KModes\n","kmodes = KModes(n_clusters=K_CLUSTERS, init='Huang', n_init=5, max_iter=20, random_state=SEED, verbose=2)\n","# Usamos 'cat_cols_index' aunque no es estrictamente necesario si ya es solo categórico\n","clusters_train = kmodes.fit_predict(X_train_array, categorical=cat_cols_index)\n","\n","# Guardar las etiquetas de cluster en el DataFrame principal\n","df_train_x[NOMBRE_ID_CLUSTER] = clusters_train\n","\n","print(f\"✅ K-Modes completado. Asignaciones añadidas en '{NOMBRE_ID_CLUSTER}'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7REMXUuucBG","executionInfo":{"status":"ok","timestamp":1763254981158,"user_tz":-60,"elapsed":187393,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"57fba94d-aeea-4206-a557-96a002b9d0d1"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iniciando K-Modes con K=50...\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run 1, iteration: 1/20, moves: 27711, cost: 217338.0\n","Run 1, iteration: 2/20, moves: 4546, cost: 216956.0\n","Run 1, iteration: 3/20, moves: 114, cost: 216956.0\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run 2, iteration: 1/20, moves: 20369, cost: 226704.0\n","Run 2, iteration: 2/20, moves: 6988, cost: 224427.0\n","Run 2, iteration: 3/20, moves: 2822, cost: 223794.0\n","Run 2, iteration: 4/20, moves: 632, cost: 223794.0\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run 3, iteration: 1/20, moves: 28850, cost: 216457.0\n","Run 3, iteration: 2/20, moves: 3641, cost: 215989.0\n","Run 3, iteration: 3/20, moves: 146, cost: 215989.0\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run 4, iteration: 1/20, moves: 26836, cost: 220030.0\n","Run 4, iteration: 2/20, moves: 5232, cost: 219594.0\n","Run 4, iteration: 3/20, moves: 288, cost: 219594.0\n","Init: initializing centroids\n","Init: initializing clusters\n","Starting iterations...\n","Run 5, iteration: 1/20, moves: 26275, cost: 219571.0\n","Run 5, iteration: 2/20, moves: 3883, cost: 218742.0\n","Run 5, iteration: 3/20, moves: 572, cost: 218742.0\n","Best run was number 3\n","✅ K-Modes completado. Asignaciones añadidas en 'cluster_id_des'.\n"]}]},{"cell_type":"code","source":["# =========================================================\n","# 4. CALCULAR Y UNIR MÉTRICAS AGREGADAS CON NOMBRES ÚNICOS\n","# =========================================================\n","\n","df_cluster_stats = df_train_x.copy()\n","df_cluster_stats['target'] = df_train_y\n","\n","# Calcular las métricas agregadas\n","cluster_metrics_cat = df_cluster_stats.groupby(NOMBRE_ID_CLUSTER)['target'].agg(\n","    # Usamos el sufijo _CAT\n","    **{f'mean_demand{SUFIJO}': 'mean',\n","       f'max_demand{SUFIJO}': 'max',\n","       f'std_sales{SUFIJO}': 'std',\n","       f'count{SUFIJO}': 'count'}\n",").reset_index()\n","\n","cluster_metrics_cat[f'std_sales{SUFIJO}'] = cluster_metrics_cat[f'std_sales{SUFIJO}'].fillna(0)\n","\n","print(f\"\\n--- Estadísticas Agregadas ({SUFIJO}) ---\")\n","print(cluster_metrics_cat.head())\n","\n","# Unir al set de entrenamiento\n","df_train_x = pd.merge(\n","    df_train_x,\n","    cluster_metrics_cat,\n","    on=NOMBRE_ID_CLUSTER,\n","    how='left'\n",")\n","\n","print(f\"\\n✅ Features de 'Neighborhood Encoding' ({SUFIJO}) añadidas a df_train_x.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E04AdbjJukO6","executionInfo":{"status":"ok","timestamp":1763254981862,"user_tz":-60,"elapsed":674,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"1f0a62d7-d9d4-47a1-bf0d-03c065463200"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Estadísticas Agregadas (_DES) ---\n","   cluster_id_des  mean_demand_DES  max_demand_DES  std_sales_DES  count_DES\n","0               0     19200.583919          106730   17575.816800       2562\n","1               1     64574.439617          242420   47299.220225       4488\n","2               2     33502.708354          223205   39938.015537       2047\n","3               3     17342.211293          128099   16330.760201       3294\n","4               4     30400.547221          138771   20645.453449       3706\n","\n","✅ Features de 'Neighborhood Encoding' (_DES) añadidas a df_train_x.\n"]}]},{"cell_type":"code","source":["# --- A. Preparación de Features (Solo Categóricas) ---\n","\n","# Crear la matriz limpia para clustering del set de prueba (solo CAT_FEATURES)\n","X_test_cluster = df_test_x[CAT_FEATURES].copy()\n","\n","# Rellenar NaNs y convertir a string, usando el mismo manejo de NaNs que en el entrenamiento\n","for col in CAT_FEATURES:\n","    X_test_cluster[col] = X_test_cluster[col].fillna('MISSING_CAT').astype(str)\n","\n","# Crear el array de NumPy para la predicción\n","X_test_array = X_test_cluster.values\n","\n","\n","# --- B. Predicción del Cluster ---\n","\n","# 1. Predecir los cluster_id usando el modelo KModes entrenado\n","# Nota: La lista 'cat_cols_index' no es necesaria en .predict() si el array es 100% categórico\n","clusters_test = kmodes.predict(X_test_array)\n","\n","# 2. Asignar el nuevo cluster ID a df_test_x\n","df_test_x[NOMBRE_ID_CLUSTER] = clusters_test\n","\n","print(f\"✅ Cluster IDs K-Modes asignados a df_test_x en columna '{NOMBRE_ID_CLUSTER}'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiZpEHTqu8bm","executionInfo":{"status":"ok","timestamp":1763254981958,"user_tz":-60,"elapsed":65,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"26eb2a9e-e6d2-42f1-e907-26ecac9dd268"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cluster IDs K-Modes asignados a df_test_x en columna 'cluster_id_des'.\n"]}]},{"cell_type":"code","source":["# --- C. Unión de las Métricas Agregadas ---\n","\n","# Renombrar la columna del cluster ID en las métricas para que coincida con df_test_x\n","# (Asumimos que cluster_metrics_cat todavía tiene el nombre 'cluster_id_cat' de la sección 4)\n","cluster_metrics_cat = cluster_metrics_cat.rename(\n","    columns={f'cluster_id{SUFIJO}': NOMBRE_ID_CLUSTER}\n",")\n","\n","\n","# 3. Unir las métricas de rendimiento del cluster (calculadas con datos de entrenamiento)\n","df_test_x = pd.merge(\n","    df_test_x,\n","    cluster_metrics_cat,\n","    on=NOMBRE_ID_CLUSTER,\n","    how='left'\n",")\n","\n","# --- D. Limpieza Final ---\n","\n","# 4. Eliminar el ID del cluster después de la unión para evitar que sea usado como feature ordinal\n","df_test_x = df_test_x.drop(columns=[NOMBRE_ID_CLUSTER], errors='ignore')\n","df_train_x = df_train_x.drop(columns=[NOMBRE_ID_CLUSTER], errors='ignore')\n","\n","print(f\"\\n✅ Features de 'Neighborhood Encoding' ({SUFIJO}) añadidas a df_test_x.\")\n","print(f\"Dimensiones de df_test_x final: {df_test_x.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvjMpz9OvOvi","executionInfo":{"status":"ok","timestamp":1763254982120,"user_tz":-60,"elapsed":150,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"1cbf550b-e616-48af-c7af-cf323e88a336"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ Features de 'Neighborhood Encoding' (_DES) añadidas a df_test_x.\n","Dimensiones de df_test_x final: (2250, 160)\n"]}]},{"cell_type":"code","source":["if 'image_embedding' in CATEGORICAL_FEATURES:\n","    CATEGORICAL_FEATURES.remove('image_embedding')\n","\n","train_pool = Pool(\n","    data=df_train_x,\n","    label=df_train_y, # Usar el target original, o df_train_y_log si se aplica la transformación\n","    cat_features=CATEGORICAL_FEATURES,\n","    thread_count=-1 # Para usar todos los cores disponibles\n",")"],"metadata":{"id":"lZIkbKkjYLNS","executionInfo":{"status":"ok","timestamp":1763255823440,"user_tz":-60,"elapsed":1567,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from catboost import CatBoostRegressor\n","\n","model = CatBoostRegressor(\n","     iterations=2500,\n","     learning_rate=0.05,\n","     depth=8,\n","     loss_function=\"RMSE\",\n","     eval_metric=\"RMSE\",\n","     random_seed=42,\n","     # Añadir Detención Temprana (Early Stopping)\n","     od_type=\"Iter\",  # Tipo de detención: por iteración\n","     od_wait=300,     # Esperar 300 iteraciones sin mejora antes de parar\n","     # Otros\n","     verbose=100\n",")\n","\n","model.fit(\n","     train_pool,  # El Pool de entrenamiento\n","     verbose=100\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHaWDHaiYPwb","executionInfo":{"status":"ok","timestamp":1763258482725,"user_tz":-60,"elapsed":2630422,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"0b26cd6d-a4dc-4c9a-ce61-0eb470ebdf13"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 33310.8143942\ttotal: 1.55s\tremaining: 1h 4m 35s\n","100:\tlearn: 6407.0096420\ttotal: 1m 40s\tremaining: 39m 48s\n","200:\tlearn: 4886.4748329\ttotal: 3m 22s\tremaining: 38m 32s\n","300:\tlearn: 3872.2806920\ttotal: 5m 6s\tremaining: 37m 20s\n","400:\tlearn: 3215.1021847\ttotal: 7m 21s\tremaining: 38m 31s\n","500:\tlearn: 2741.1905043\ttotal: 9m 4s\tremaining: 36m 10s\n","600:\tlearn: 2381.3999027\ttotal: 10m 47s\tremaining: 34m 4s\n","700:\tlearn: 2079.5269837\ttotal: 12m 32s\tremaining: 32m 10s\n","800:\tlearn: 1843.2142964\ttotal: 14m 17s\tremaining: 30m 18s\n","900:\tlearn: 1649.9964657\ttotal: 16m 2s\tremaining: 28m 28s\n","1000:\tlearn: 1486.4062007\ttotal: 17m 48s\tremaining: 26m 39s\n","1100:\tlearn: 1347.5132573\ttotal: 19m 31s\tremaining: 24m 48s\n","1200:\tlearn: 1230.5162432\ttotal: 21m 16s\tremaining: 23m\n","1300:\tlearn: 1131.7190062\ttotal: 22m 59s\tremaining: 21m 11s\n","1400:\tlearn: 1045.1184759\ttotal: 24m 42s\tremaining: 19m 23s\n","1500:\tlearn: 967.9210182\ttotal: 26m 26s\tremaining: 17m 35s\n","1600:\tlearn: 898.3293738\ttotal: 28m 12s\tremaining: 15m 50s\n","1700:\tlearn: 842.8526739\ttotal: 29m 56s\tremaining: 14m 3s\n","1800:\tlearn: 791.6117244\ttotal: 31m 40s\tremaining: 12m 17s\n","1900:\tlearn: 747.7820327\ttotal: 33m 22s\tremaining: 10m 30s\n","2000:\tlearn: 711.4932257\ttotal: 35m 5s\tremaining: 8m 44s\n","2100:\tlearn: 676.2237467\ttotal: 36m 52s\tremaining: 7m\n","2200:\tlearn: 645.1139005\ttotal: 38m 36s\tremaining: 5m 14s\n","2300:\tlearn: 618.4627547\ttotal: 40m 21s\tremaining: 3m 29s\n","2400:\tlearn: 594.1189529\ttotal: 42m 3s\tremaining: 1m 44s\n","2499:\tlearn: 573.8685493\ttotal: 43m 46s\tremaining: 0us\n"]},{"output_type":"execute_result","data":{"text/plain":["<catboost.core.CatBoostRegressor at 0x7a77bf6273b0>"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["test_prod = df_test_x.copy()\n","test_predictions = model.predict(df_test_x)"],"metadata":{"id":"Z0RuYaGTY06D","executionInfo":{"status":"ok","timestamp":1763258482980,"user_tz":-60,"elapsed":240,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame({\n","    \"ID\": test_prod[\"ID\"],\n","    \"Production\": test_predictions\n","})\n","\n","submission.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"uNN1KoJ_Y3z_","executionInfo":{"status":"ok","timestamp":1763258483044,"user_tz":-60,"elapsed":40,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["LightBoost"],"metadata":{"id":"HuEbk2kQCdMH"}},{"cell_type":"code","source":["import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","\n","\n","CATEGORICAL_COLS = df_train_x.select_dtypes(include=['object', 'category']).columns.tolist()\n","BEST_ITERATION = 500\n","\n","params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31,\n","    \"feature_fraction\": 0.8,\n","    \"bagging_fraction\": 0.8,\n","    \"bagging_freq\": 1,\n","    \"verbose\": 2,\n","    \"seed\": 42,\n","}\n","\n","# 1. Asegúrate de que las columnas sean del tipo 'category' en Pandas\n","for col in CATEGORICAL_COLS:\n","    df_train_x[col] = df_train_x[col].astype(\"category\")\n","    df_test_x[col] = df_test_x[col].astype(\"category\")\n","\n","\n","# 2. Pasar la lista de columnas categóricas a lgb.Dataset\n","lgb_full = lgb.Dataset(\n","    df_train_x,\n","    label=df_train_y,\n","    categorical_feature=CATEGORICAL_COLS, # <-- Se pasa la lista de los nombres de columnas\n","    free_raw_data=False\n",")\n","\n","print(\"[FINAL] Entrenando modelo LightGBM...\")\n","model_full = lgb.train(\n","    params,\n","    lgb_full,\n","    num_boost_round=BEST_ITERATION # Usar BEST_ITERATION\n",")\n","\n","print(\"[FINAL] Prediciendo sobre test...\")\n","preds = model_full.predict(\n","    df_test_x,\n","    num_iteration=BEST_ITERATION # Usar BEST_ITERATION\n",")\n","preds = np.maximum(preds, 0) # evitar negativos\n","\n","# Nota: Asegúrate de que 'sample_submission.csv' es el archivo de la estructura, no el test.csv\n","sample_sub = pd.read_csv('/content/drive/MyDrive/Datathon/sample_submission.csv')\n","submission = sample_sub.copy()\n","submission[\"Production\"] = preds\n","\n","print(\"✅ Predicción final generada.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dn162EikCcXV","executionInfo":{"status":"ok","timestamp":1763258544941,"user_tz":-60,"elapsed":61753,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"030b8af5-8feb-49bc-d845-8f95e86089f5"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["[FINAL] Entrenando modelo LightGBM...\n","[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n","[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n","[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.804072\n","[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.039242\n","[LightGBM] [Debug] init for col-wise cost 0.008598 seconds, init for row-wise cost 0.147893 seconds\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198525 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42535\n","[LightGBM] [Info] Number of data points in the train set: 95339, number of used features: 160\n","[LightGBM] [Info] Start training from score 28927.421055\n","[LightGBM] [Debug] Re-bagging, using 76157 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76209 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76395 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n","[LightGBM] [Debug] Re-bagging, using 76233 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76094 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76248 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76360 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76233 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76019 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76044 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76053 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76152 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76148 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76161 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76352 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76418 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76402 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76273 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76384 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76327 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76082 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76393 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76408 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76352 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76465 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76311 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76319 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76514 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76222 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n","[LightGBM] [Debug] Re-bagging, using 75943 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76199 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76255 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76274 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76377 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76230 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76413 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76334 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76319 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76327 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 75915 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76283 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76173 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76274 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76129 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76241 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76505 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76312 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76266 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76228 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76444 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76031 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76460 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76437 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76306 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76234 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76196 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76265 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76105 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76467 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76154 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76202 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76292 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76363 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76272 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76241 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76177 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76322 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76217 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76325 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76275 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76397 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76493 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76366 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76171 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76155 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76388 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76373 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76227 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76237 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76438 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76219 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76258 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76150 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76196 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76336 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76358 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76342 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76306 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76105 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76361 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76116 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76240 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76418 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76161 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76114 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76292 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76084 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76446 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76459 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76428 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76462 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76264 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76491 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76390 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76187 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76316 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76388 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76284 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76365 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76340 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76267 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76377 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76369 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76317 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76443 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76325 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76366 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76003 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76214 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76348 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76353 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 19\n","[LightGBM] [Debug] Re-bagging, using 76318 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76258 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76090 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76160 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76255 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76170 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76309 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76306 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76347 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76316 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76271 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76326 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76408 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76264 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 22\n","[LightGBM] [Debug] Re-bagging, using 76645 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76275 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76444 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76435 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76369 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76310 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76144 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76328 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76341 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76345 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76462 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76179 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76358 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76300 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76273 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76480 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 75874 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76317 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76486 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76257 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76116 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76212 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76458 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76304 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76431 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76482 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76237 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76359 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76168 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76070 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76368 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76405 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76487 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76333 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76116 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76253 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76212 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76090 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76415 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76072 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76264 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76161 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76363 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76391 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76267 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76215 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76551 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76236 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76388 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76200 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76109 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76290 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76513 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76290 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76194 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76295 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76342 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76470 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76283 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76158 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76336 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76144 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76324 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76151 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76383 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76145 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76283 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76262 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76255 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76390 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76369 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76095 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76263 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76366 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76311 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76157 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76232 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76359 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76145 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76115 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76228 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76141 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76416 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 21\n","[LightGBM] [Debug] Re-bagging, using 76391 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76212 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76625 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76093 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76420 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76141 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n","[LightGBM] [Debug] Re-bagging, using 76452 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76112 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76374 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76167 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76366 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76340 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 19\n","[LightGBM] [Debug] Re-bagging, using 76144 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76233 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n","[LightGBM] [Debug] Re-bagging, using 75954 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76201 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76317 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76123 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76222 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76198 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76346 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76458 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76430 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76449 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76454 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76228 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76388 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76113 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76205 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76340 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76023 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76297 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76310 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76294 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n","[LightGBM] [Debug] Re-bagging, using 76464 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76266 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76348 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76310 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76234 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76271 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76366 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76467 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76381 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76299 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76226 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76264 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76082 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76252 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76404 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76279 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76177 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76365 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76426 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76361 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76236 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76063 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76336 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76397 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76455 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76180 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76152 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76209 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76368 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76478 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76466 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76315 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76211 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76295 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76258 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76294 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76163 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76153 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76153 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76454 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76492 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76301 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76259 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76248 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76195 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76201 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76244 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76234 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76152 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76371 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76235 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76258 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 19\n","[LightGBM] [Debug] Re-bagging, using 76243 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76208 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76260 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76448 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76300 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76252 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76212 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76367 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76021 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76195 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76294 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76292 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76361 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76230 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76021 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76371 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76318 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76091 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76337 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76285 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76324 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76184 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76315 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76022 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76448 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76200 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76117 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76384 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76205 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76184 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76178 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76255 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76451 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76333 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76304 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76485 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76273 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76319 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76107 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76263 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76161 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76513 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76245 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76511 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76039 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76055 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76373 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76440 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76318 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76211 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76417 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76269 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76485 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76423 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76361 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76213 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76398 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76211 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n","[LightGBM] [Debug] Re-bagging, using 76211 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76060 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76380 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76230 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76033 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76234 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76243 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76146 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76400 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76393 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76105 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76221 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76295 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76408 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76181 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76321 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76341 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76082 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76342 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76426 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76532 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76232 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76347 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76182 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76085 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76113 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76436 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76165 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76328 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76272 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76217 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76374 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76173 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76174 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76403 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76315 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76210 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 21\n","[LightGBM] [Debug] Re-bagging, using 76365 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76125 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76108 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76399 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76199 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76009 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76215 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76275 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76104 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76272 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76357 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76279 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76159 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76276 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76469 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76423 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76080 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76205 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76188 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76285 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76319 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76104 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76134 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76342 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76197 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76388 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76435 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n","[LightGBM] [Debug] Re-bagging, using 76095 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76311 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76514 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76034 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76503 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76348 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76438 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76436 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76256 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76294 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76204 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76247 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76399 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76091 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76203 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76456 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76282 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76137 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76164 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76395 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76333 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76356 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76283 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76273 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n","[LightGBM] [Debug] Re-bagging, using 76358 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76260 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76075 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 19\n","[LightGBM] [Debug] Re-bagging, using 76334 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76441 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76235 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76533 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76168 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76290 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76177 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76508 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76210 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76185 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76287 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76479 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n","[LightGBM] [Debug] Re-bagging, using 76078 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76493 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n","[LightGBM] [Debug] Re-bagging, using 76217 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76382 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76027 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76297 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76428 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76152 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 76329 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76490 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n","[LightGBM] [Debug] Re-bagging, using 76532 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76098 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76209 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76435 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76280 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76178 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76203 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76079 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n","[LightGBM] [Debug] Re-bagging, using 76180 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76115 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76394 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76126 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n","[LightGBM] [Debug] Re-bagging, using 76261 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n","[LightGBM] [Debug] Re-bagging, using 76186 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[LightGBM] [Debug] Re-bagging, using 76266 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76140 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76199 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76274 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76018 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n","[LightGBM] [Debug] Re-bagging, using 76465 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n","[LightGBM] [Debug] Re-bagging, using 76299 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n","[LightGBM] [Debug] Re-bagging, using 75946 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n","[LightGBM] [Debug] Re-bagging, using 76277 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n","[FINAL] Prediciendo sobre test...\n","✅ Predicción final generada.\n"]}]},{"cell_type":"code","source":["test_predictions = submission[\"Production\"]\n","test_prod = submission[\"ID\"]"],"metadata":{"id":"9m2HVpciGmsJ","executionInfo":{"status":"ok","timestamp":1763258544983,"user_tz":-60,"elapsed":5,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","# ... (Tu código de entrenamiento y predicción) ...\n","\n","# -------------------------------------------------------------\n","# Zona de la Predicción Final (Tu código original)\n","# -------------------------------------------------------------\n","# ... (Tu código de predicción, que calcula 'preds') ...\n","\n","# Nota: Asegúrate de que 'sample_submission.csv' es el archivo de la estructura, no el test.csv\n","sample_sub = pd.read_csv('/content/drive/MyDrive/Datathon/sample_submission.csv')\n","submission = sample_sub.copy()\n","submission[\"Production\"] = preds # El DataFrame 'submission' ya tiene las columnas \"ID\" y \"Production\"\n","\n","print(\"✅ Predicción final generada.\")\n","\n","# --- INICIO DEL CÓDIGO PROBLEMÁTICO Y CORRECCIÓN ---\n","\n","# [ELIMINAR ESTAS LÍNEAS O DEJAR LA SUBMISSION DIRECTA]\n","# test_predictions = submission[\"Production\"]\n","# test_prod = submission[\"ID\"]\n","# submission = pd.DataFrame({\n","#     \"ID\": test_prod[\"ID\"],\n","#     \"Production\": test_predictions\n","# })\n","# [FIN DEL CÓDIGO PROBLEMATICO]\n","\n","# La variable 'submission' en este punto ya tiene el formato correcto:\n","# submission.head() mostrará columnas ID y Production\n","\n","submission.to_csv(\"submission_light.csv\", index=False)\n","print(\"✅ Submission guardada correctamente.\")"],"metadata":{"id":"DgtVZy47aZ9q","executionInfo":{"status":"ok","timestamp":1763258775265,"user_tz":-60,"elapsed":170,"user":{"displayName":"Norman Castro","userId":"10364213291937823557"}},"outputId":"c05ec08e-a1ce-4bad-bb42-b71306caf24d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Predicción final generada.\n","✅ Submission guardada correctamente.\n"]}]}]}